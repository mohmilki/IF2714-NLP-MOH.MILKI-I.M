{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7u3cKVlnFmm1Az9bj9U7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Membuat Chatbot Sederhana Online Shop"],"metadata":{"id":"_ttmswYmZvU_"}},{"cell_type":"markdown","source":["Import beberapa library yang diperlukan untuk pembuatan chatbot"],"metadata":{"id":"Df_FAoiYZ5yk"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_gk0jgeU6E2b","executionInfo":{"status":"ok","timestamp":1673148263567,"user_tz":-420,"elapsed":400,"user":{"displayName":"moh milki","userId":"12912637772169725910"}}},"outputs":[],"source":["from IPython.core.display import Image, display"]},{"cell_type":"code","source":["#Used in Tensorflow Model\n","import numpy as np\n","import tensorflow as tf\n","!pip install tflearn \n","import tflearn\n","import random\n","\n","#Usde to for Contextualisation and Other NLP Tasks.\n","import nltk\n","nltk.download('punkt')\n","from nltk.stem.lancaster import LancasterStemmer\n","stemmer = LancasterStemmer()\n","\n","#Other\n","import json\n","import pickle\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPHHj0Ji6OF0","executionInfo":{"status":"ok","timestamp":1673148276039,"user_tz":-420,"elapsed":9937,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"5370846e-d5ee-4750-a0ef-4795d6fdd73e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tflearn\n","  Downloading tflearn-0.5.0.tar.gz (107 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.3 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=cadcbd83802553c673b5d3ca566ea2dc476bb26d0e9181340706837170a721a2\n","  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.5.0\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["Kemudian, import drive agar data bisa diambil dari google drive."],"metadata":{"id":"aeX0vH9SaP6T"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQX3JVUM6cPI","executionInfo":{"status":"ok","timestamp":1673148358647,"user_tz":-420,"elapsed":21992,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"23b153a6-4618-428a-cb06-bde4103686d2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Mengambil data data-uas.json dan menyimpannya pada variabel dengan nama f"],"metadata":{"id":"XF_nps73aTds"}},{"cell_type":"code","source":["f = open('./drive/MyDrive/Colab Notebooks/nlp/UAS/data-uas.json', 'r')"],"metadata":{"id":"DBkF6YWe6jn6","executionInfo":{"status":"ok","timestamp":1673148363213,"user_tz":-420,"elapsed":755,"user":{"displayName":"moh milki","userId":"12912637772169725910"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Memproses data 'data-uas.json' dan menyimpannya ke dalam variabel intents"],"metadata":{"id":"ls_QAgMCagOM"}},{"cell_type":"code","source":["intents = json.load(f)"],"metadata":{"id":"UCkswjWq6vyN","executionInfo":{"status":"ok","timestamp":1673148407882,"user_tz":-420,"elapsed":899,"user":{"displayName":"moh milki","userId":"12912637772169725910"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Membuat variabel array dengan nama words, classes, documents, dan ignore_words, serta membuat perulangan for untuk proses tokenize (pemisahan) dan pengelompokkan."],"metadata":{"id":"9SX6p8oGaqp3"}},{"cell_type":"code","source":["words = []\n","classes = []\n","documents = []\n","ignore_words = ['?']\n","print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        # tokenize each word in the sentence\n","        w = nltk.word_tokenize(pattern)\n","        # add to our words list\n","        words.extend(w)\n","        # add to documents in our corpus\n","        documents.append((w, intent['tag']))\n","        # add to our classes list\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkkWkvo46x7i","executionInfo":{"status":"ok","timestamp":1673148524780,"user_tz":-420,"elapsed":423,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"8a42edeb-cb50-4660-a7a2-1865e064e46c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"]}]},{"cell_type":"markdown","source":["Proses proses stemming (pengubah setiap kata menjadi kata dasar), lowering (mengubah setiap huruf menjadi huruf kecil) dan duplicates (menghilangkan huruf yang ganda)."],"metadata":{"id":"MMBbxSMMbMo5"}},{"cell_type":"code","source":["print(\"Stemming, Lowering and Removing Duplicates.......\")\n","words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","# remove duplicates\n","classes = sorted(list(set(classes)))\n","\n","print (len(documents), \"documents\")\n","print (len(classes), \"classes\", classes)\n","print (len(words), \"unique stemmed words\", words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pqFG1pcZ7Pc5","executionInfo":{"status":"ok","timestamp":1673148531951,"user_tz":-420,"elapsed":426,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"cef47c13-c54c-4b7c-d02a-22da973c6d37"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemming, Lowering and Removing Duplicates.......\n","26 documents\n","9 classes ['goodbye', 'greeting', 'hours', 'name', 'place', 'products', 'products_detail', 'products_price', 'thanks']\n","39 unique stemmed words [\"'s\", 'any', 'anyon', 'ar', 'bye', 'day', 'do', 'from', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'is', 'it', 'kind', 'lat', 'many', 'much', 'nam', 'of', 'op', 'produc', 'school', 'see', 'shop', 'thank', 'that', 'ther', 'thi', 'uniform', 'what', 'when', 'wher', 'yo', 'you']\n"]}]},{"cell_type":"markdown","source":["# Proses Pembuatan data dan Modelling"],"metadata":{"id":"OGA3iIkAbplY"}},{"cell_type":"code","source":["print(\"Creating the Data for our Model.....\")\n","training = []\n","output = []\n","print(\"Creating an List (Empty) for Output.....\")\n","output_empty = [0] * len(classes)\n","\n","print(\"Creating Traning Set, Bag of Words for our Model....\")\n","for doc in documents:\n","    # initialize our bag of words\n","    bag = []\n","    # list of tokenized words for the pattern\n","    pattern_words = doc[0]\n","    # stem each word\n","    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n","    # create our bag of words array\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","\n","    # output is a '0' for each tag and '1' for current tag\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_CMK7Qm7ROy","executionInfo":{"status":"ok","timestamp":1673148688185,"user_tz":-420,"elapsed":705,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"a9496d88-8e6f-4204-ffb5-28577356e78d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating the Data for our Model.....\n","Creating an List (Empty) for Output.....\n","Creating Traning Set, Bag of Words for our Model....\n"]}]},{"cell_type":"code","source":["print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n","random.shuffle(training)\n","training = np.array(training)\n","\n","print(\"Creating Train and Test Lists.....\")\n","train_x = list(training[:,0])\n","train_y = list(training[:,1])\n","print(\"Building Neural Network for Out Chatbot to be Contextual....\")\n","print(\"Resetting graph data....\")\n","tf.compat.v1.reset_default_graph()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4g4ENyY7Tm9","executionInfo":{"status":"ok","timestamp":1673148690566,"user_tz":-420,"elapsed":346,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"ef201d6a-b3f0-4bc9-8d12-bfc8d73f3313"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n","Creating Train and Test Lists.....\n","Building Neural Network for Out Chatbot to be Contextual....\n","Resetting graph data....\n"]}]},{"cell_type":"code","source":["net = tflearn.input_data(shape=[None, len(train_x[0])])\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, 8)\n","net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n","net = tflearn.regression(net)\n","print(\"Training....\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJb3J3Q37Vmo","executionInfo":{"status":"ok","timestamp":1673148692807,"user_tz":-420,"elapsed":8,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"c6901a3b-c82a-47cb-c339-7ad378d6783a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"]},{"output_type":"stream","name":"stdout","text":["Training....\n"]}]},{"cell_type":"code","source":["model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"],"metadata":{"id":"3tPv5PLS7XQs","executionInfo":{"status":"ok","timestamp":1673148695987,"user_tz":-420,"elapsed":1030,"user":{"displayName":"moh milki","userId":"12912637772169725910"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Proses Training Model"],"metadata":{"id":"SjkuE_pwbxO6"}},{"cell_type":"code","source":["print(\"Training the Model.......\")\n","model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n","print(\"Saving the Model.......\")\n","model.save('model.tflearn')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbhxvpe37ZIO","executionInfo":{"status":"ok","timestamp":1673148815334,"user_tz":-420,"elapsed":103485,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"156e24e8-2b92-4e07-8daa-85f5fc67bf20"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.00365\u001b[0m\u001b[0m | time: 0.019s\n","| Adam | epoch: 1000 | loss: 0.00365 - acc: 1.0000 -- iter: 24/26\n","Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.00376\u001b[0m\u001b[0m | time: 0.022s\n","| Adam | epoch: 1000 | loss: 0.00376 - acc: 1.0000 -- iter: 26/26\n","--\n","Saving the Model.......\n"]}]},{"cell_type":"code","source":["print(\"Pickle is also Saved..........\")\n","pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKISpFUt7ads","executionInfo":{"status":"ok","timestamp":1673148852850,"user_tz":-420,"elapsed":361,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"12cca7cd-4c9a-4821-db32-25d0d5b639f5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Pickle is also Saved..........\n"]}]},{"cell_type":"code","source":["print(\"Loading Pickle.....\")\n","data = pickle.load( open( \"training_data\", \"rb\" ) )\n","words = data['words']\n","classes = data['classes']\n","train_x = data['train_x']\n","train_y = data['train_y']\n","\n","f = open('./drive/MyDrive/Colab Notebooks/nlp/UAS/data-uas.json', 'r')\n","intents = json.load(f)\n","\n","\n","print(\"Loading the Model......\")\n","# load our saved model\n","model.load('./model.tflearn')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTWsjMpW72kn","executionInfo":{"status":"ok","timestamp":1673148853621,"user_tz":-420,"elapsed":9,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"1c71dc02-69db-4a63-d514-f1fef0edc408"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Pickle.....\n","Loading the Model......\n"]}]},{"cell_type":"markdown","source":["# Membuat Fungsi-fungsi untuk mengilah input data yang diberikan oleh input user pada chatbot"],"metadata":{"id":"ukYD0kgQcaGC"}},{"cell_type":"code","source":["def clean_up_sentence(sentence):\n","    # It Tokenize or Break it into the constituents parts of Sentense.\n","    sentence_words = nltk.word_tokenize(sentence)\n","    # Stemming means to find the root of the word.\n","    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n","def bow(sentence, words, show_details=False):\n","    sentence_words = clean_up_sentence(sentence)\n","    bag = [0]*len(words)\n","    for s in sentence_words:\n","        for i,w in enumerate(words):\n","            if w == s:\n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","    return(np.array(bag))\n","\n","ERROR_THRESHOLD = 0.25\n","print(\"ERROR_THRESHOLD = 0.25\")\n","\n","def classify(sentence):\n","    # Prediction or To Get the Posibility or Probability from the Model\n","    results = model.predict([bow(sentence, words)])[0]\n","    # Exclude those results which are Below Threshold\n","    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n","    # Sorting is Done because heigher Confidence Answer comes first.\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n","    return return_list\n","\n","def response(sentence, userID='123', show_details=False):\n","    results = classify(sentence)\n","    # That Means if Classification is Done then Find the Matching Tag.\n","    if results:\n","        # Long Loop to get the Result.\n","        while results:\n","            for i in intents['intents']:\n","                # Tag Finding\n","                if i['tag'] == results[0][0]:\n","                    # Random Response from High Order Probabilities\n","                    return print(random.choice(i['responses']))\n","\n","            results.pop(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_7e499r8fDc","executionInfo":{"status":"ok","timestamp":1673148992760,"user_tz":-420,"elapsed":384,"user":{"displayName":"moh milki","userId":"12912637772169725910"}},"outputId":"c7304932-d033-4aa7-8086-01af930c5971"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["ERROR_THRESHOLD = 0.25\n"]}]},{"cell_type":"markdown","source":["# Testing Chatbot"],"metadata":{"id":"boSXr4FZc6bM"}},{"cell_type":"code","source":["while True:\n","    input_data = input(\"Customer- \")\n","    answer = response(input_data)\n","    answer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jp7BIkth8iNz","outputId":"e47f7ffd-6707-44bc-ff8f-5dcd3ad5cdcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Customer- what's your name?\n","My name is Mimi\n","Customer- Where are you from?\n","Bye! COme back again soon\n","Customer- Where is your shop?\n","We are from Kp. Saladah, Kec. Padakembang, Tasikmalaya, West java, Indonesia\n","Customer- What hours are you open?\n","We're open from monday to saturday from 08am-10pm\n","Customer- \"What kind of products do you have?\n","We have varies of Clothes.\n","Customer- How many school uniforms do you have?\n","yes, we have many schools uniforms here\n","Customer- Thank you\n","Any time!\n","Customer- thank's\n","My Pleasure\n"]}]}]}